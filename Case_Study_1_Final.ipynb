{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Case_Study_1_Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3GV5CKnQLG1"
      },
      "source": [
        "<p style=\"font-size:32px;text-align:center\"> <b>Amazon.com - Employee Access Challenge</b> </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o50ab6vJQaAs"
      },
      "source": [
        "### Problem statement: \n",
        "Predict an employee's access needs, given his/her job role"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ukJhCvmQcbe"
      },
      "source": [
        "### Problem Description:\n",
        "When an employee at any company starts work, they first need to obtain the computer access necessary to fulfill their role. This access may allow an employee to read/manipulate resources through various applications or web portals. It is assumed that employees fulfilling the functions of a given role will access the same or similar resources. It is often the case that employees figure out the access they need as they encounter roadblocks during their daily work (e.g. not able to log into a reporting portal). A knowledgeable supervisor then takes time to manually grant the needed access in order to overcome access obstacles. As employees move throughout a company, this access discovery/recovery cycle wastes a nontrivial amount of time and money.\n",
        "\n",
        "There is a considerable amount of data regarding an employeeâ€™s role within an organization and the resources to which they have access. Given the data related to current employees and their provisioned access, models can be built that automatically determine access privileges as employees enter and leave roles within a company. These auto-access models seek to minimize the human involvement required to grant or revoke employee access."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSge_JxoQcfL"
      },
      "source": [
        "### Data Overview\n",
        "Taken data from Amazon.com - Employee Access Challenge on kaggle https://www.kaggle.com/c/amazon-employee-access-challenge\n",
        "The data consists of real historical data collected from 2010 & 2011.  Employees are manually allowed or denied access to resources over time. You must create an algorithm capable of learning from this historical data to predict approval/denial for an unseen set of employees. \n",
        "    - Data columns (total 10 columns):  \n",
        "\n",
        "\n",
        "<table>\n",
        "  <col width=\"130\">\n",
        "  <col width=\"10\">\n",
        "  <tr>\n",
        "    <th>Column Name</th>\n",
        "    <th>Description</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>ACTION</td>\n",
        "    <td>ACTION is 1 if the resource was approved, 0 if the resource was not</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>RESOURCE</td>\n",
        "    <td>An ID for each resource</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>MGR_ID</td>\n",
        "    <td>The EMPLOYEE ID of the manager of the current EMPLOYEE ID record; an employee may have only one manager at a time</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>ROLE_ROLLUP_1</td>\n",
        "    <td>Company role grouping category id 1 (e.g. US Engineering)</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>ROLE_ROLLUP_2</td>\n",
        "    <td>Company role grouping category id 2 (e.g. US Retail)</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>ROLE_DEPTNAME</td>\n",
        "    <td>Company role department description (e.g. Retail)</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>ROLE_TITLE</td>\n",
        "    <td>Company role business title description (e.g. Senior Engineering Retail Manager)</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>ROLE_FAMILY_DESC</td>\n",
        "    <td>Company role family extended description (e.g. Retail Manager, Software Engineering)</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>ROLE_FAMILY</td>\n",
        "    <td>Company role family description (e.g. Retail Manager)\n",
        "</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>ROLE_CODE</td>\n",
        "    <td>Company role code; this code is unique to each role (e.g. Manager)\n",
        "</td>\n",
        "  </tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP3ypG7qQcil"
      },
      "source": [
        "### Mapping the problem into supervised learning problem:\n",
        "- Generated new features like Pair wise column featurization. Considered pairs of 2 columns as well as pairs of 3 columns, and analysed if these feature pairs help to improve the model prediction. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDG3DbdQQcl2"
      },
      "source": [
        "### Business objectives and constraints:  \n",
        "- The objective of this competition is to build a model, learned using historical data, that will determine an employee's access needs, such that manual access transactions (grants and revokes) are minimized as the employee's attributes change over time. The model will take an employee's role information and a resource code and will return whether or not access should be granted.\n",
        "- Latency is not an issue here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq0iIJO3QcpN"
      },
      "source": [
        "### Performance metric for supervised learning:  \n",
        "- AUC (As mentioned in the competition evaluation criteria)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5zYJtiDGdjg"
      },
      "source": [
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfTransformer \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer #Equivalent to CountVectorizer followed by TfidfTransformer.\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer ##Convert a collection of text documents to a matrix of token counts\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "import re\n",
        "# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import os\n",
        "from pandas import HDFStore,DataFrame\n",
        "from pandas import read_hdf\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from itertools import combinations\n",
        "import xgboost as xgb\n",
        "from sklearn import metrics\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gjg5DoCxHU44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92e5ca50-d898-4552-fc60-3a41d043f54c"
      },
      "source": [
        "%matplotlib inline\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7y13f1VWImH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ee37dd6-0674-4ef2-ab12-57eb454b3083"
      },
      "source": [
        "# Changing to the required location in the drive\n",
        "%cd '/content/drive/My Drive/amazon-employee-access-challenge/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/amazon-employee-access-challenge\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Myo8LcwIlbHy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5afc4dfd-174e-4a22-a39f-9009cbb317ca"
      },
      "source": [
        "# https://stackoverflow.com/questions/41186565/python-creating-user-input-form-and-converting-inputs-into-data-frame?rq=1\n",
        "\n",
        "df = pd.DataFrame(columns=['RESOURCE', 'MGR_ID', 'ROLE_ROLLUP_1', 'ROLE_ROLLUP_2', 'ROLE_DEPTNAME', 'ROLE_TITLE', 'ROLE_FAMILY_DESC', 'ROLE_FAMILY', 'ROLE_CODE'])\n",
        "parts = int(input(\"Enter the number of rows:\"))\n",
        "\n",
        "for i in range(parts):\n",
        "    print('Taking input for row : ',int(i)+1)\n",
        "    RESOURCE = input(\"Enter RESOURCE : \")\n",
        "    MGR_ID = input(\"Enter MGR_ID : \")\n",
        "    ROLE_ROLLUP_1 = input(\"Enter ROLE_ROLLUP_1 : \")\n",
        "    ROLE_ROLLUP_2 = input(\"Enter ROLE_ROLLUP_2 : \")\n",
        "    ROLE_DEPTNAME = input(\"Enter ROLE_DEPTNAME : \")\n",
        "    ROLE_TITLE = input(\"Enter ROLE_TITLE : \")\n",
        "    ROLE_FAMILY_DESC = input(\"Enter ROLE_TITLE : \")\n",
        "    ROLE_FAMILY = input(\"Enter ROLE_FAMILY : \")\n",
        "    ROLE_CODE = input(\"Enter ROLE_CODE : \")\n",
        "\n",
        "    print(\"=\"*20)\n",
        "\n",
        "\n",
        "    df1 = pd.DataFrame(data=[[RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTNAME, ROLE_TITLE, ROLE_FAMILY_DESC, ROLE_FAMILY, ROLE_CODE]],columns=['RESOURCE', 'MGR_ID', 'ROLE_ROLLUP_1', 'ROLE_ROLLUP_2', 'ROLE_DEPTNAME', 'ROLE_TITLE', 'ROLE_FAMILY_DESC', 'ROLE_FAMILY', 'ROLE_CODE'])\n",
        "    df = pd.concat([df,df1], axis=0)\n",
        "\n",
        "df.index = range(len(df.index))\n",
        "print(df)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the number of rows:2\n",
            "Taking input for row :  1\n",
            "Enter RESOURCE : 39353\n",
            "Enter MGR_ID : 85475\n",
            "Enter ROLE_ROLLUP_1 : 117961\n",
            "Enter ROLE_ROLLUP_2 : 118300\n",
            "Enter ROLE_DEPTNAME : 123472\n",
            "Enter ROLE_TITLE : 117905\n",
            "Enter ROLE_TITLE : 117906\n",
            "Enter ROLE_FAMILY : 290919\n",
            "Enter ROLE_CODE : 117908\n",
            "====================\n",
            "Taking input for row :  2\n",
            "Enter RESOURCE : 45333\n",
            "Enter MGR_ID : 14561\n",
            "Enter ROLE_ROLLUP_1 : 117951\n",
            "Enter ROLE_ROLLUP_2 : 117952\n",
            "Enter ROLE_DEPTNAME : 118008\n",
            "Enter ROLE_TITLE : 118568\n",
            "Enter ROLE_TITLE : 118568\n",
            "Enter ROLE_FAMILY : 19721\n",
            "Enter ROLE_CODE : 118570\n",
            "====================\n",
            "  RESOURCE MGR_ID ROLE_ROLLUP_1  ... ROLE_FAMILY_DESC ROLE_FAMILY ROLE_CODE\n",
            "0    39353  85475        117961  ...           117906      290919    117908\n",
            "1    45333  14561        117951  ...           118568       19721    118570\n",
            "\n",
            "[2 rows x 9 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSMQt9joajii"
      },
      "source": [
        "\n",
        "# https://stackoverflow.com/questions/58020251/how-to-save-classifier-in-sklearn-with-countvectorizer-and-tfidftransformer\n",
        "def final_fun_1(X):\n",
        "  df = pd.DataFrame(X, columns = ['RESOURCE', 'MGR_ID', 'ROLE_ROLLUP_1', 'ROLE_ROLLUP_2', 'ROLE_DEPTNAME','ROLE_TITLE','ROLE_FAMILY_DESC', 'ROLE_FAMILY', 'ROLE_CODE'])\n",
        "  result_test = df.drop(columns=['ROLE_TITLE'])\n",
        "\n",
        "  print('Data preprocessing- Pair Wise featurization (Pairs of 2 and Pairs of 3).....')\n",
        "  # https://stackoverflow.com/questions/43347939/all-possible-combinations-of-columns-in-dataframe-pandas-python\n",
        "  # this helper function combines columns in pairs of 2. We will find out if this feature engineering technique creates a better ML model\n",
        "  # the result of pairing is columnX_columnY\n",
        "  def group_data_2(res):\n",
        "    df = res\n",
        "    cc = list(combinations(df.columns,2))\n",
        "    df = pd.concat([df[c[0]] + '_' + df[c[1]] for c in cc], axis=1, keys=cc)\n",
        "    df.columns = df.columns.map('_'.join)\n",
        "    return df\n",
        "\n",
        "  # this function combines columns in pair of 3\n",
        "  # the result of pairing is columnX_columnY_columnZ\n",
        "  def group_data_3(res):\n",
        "    df = res\n",
        "    cc = list(combinations(df.columns,3))\n",
        "    df = pd.concat([df[c[0]] + '_' + df[c[1]] + '_' + df[c[2]] for c in cc], axis=1, keys=cc)\n",
        "    df.columns = df.columns.map('_'.join)\n",
        "    return df\n",
        "\n",
        "\n",
        "  # Pair-wise featurization of Test data\n",
        "  # feature engineering: combining columns in pairs of 2\n",
        "  # the result is columnX_columnY and then concatinating all the singular columns with the resultant pairwise combination.\n",
        "  result2 = group_data_2(result_test)\n",
        "  result3 = group_data_3(result_test)\n",
        "  frames = [result_test,result2,result3]\n",
        "  result_test = pd.concat(frames,axis=1)\n",
        "\n",
        "  # The total columns should be 8C2 + 8C3 + 8 = 92\n",
        "\n",
        "  # Storing the column names in a list, we will use this later in SVD and countVectorization\n",
        "  list_col = list(result_test.columns)\n",
        "\n",
        "  print(\"Frequency Encoding the test data......\")\n",
        "  pickle_in = open(\"freq_frame.pickle\",\"rb\")\n",
        "  data_frame = pickle.load(pickle_in)\n",
        "  pickle_in.close()\n",
        "\n",
        "  test_X = result_test.copy()\n",
        "  new_dataset = pd.DataFrame()\n",
        "  for col in test_X.columns:\n",
        "    freq = (test_X[[col]].merge(data_frame[col], on = col, how = 'left'))[col+\"_freq\"]\n",
        "    new_dataset[col+\"_freq\"] = freq\n",
        "  new_dataset = new_dataset.fillna(0)\n",
        "  test_freq = new_dataset.copy()\n",
        "\n",
        "  print('Target Encoding test data....')\n",
        "  pickle_in = open(\"data_frame.pickle\",\"rb\")\n",
        "  data_frame = pickle.load(pickle_in)\n",
        "  pickle_in.close()\n",
        "\n",
        "  pickle_in = open(\"global_mean.pickle\",\"rb\")\n",
        "  global_mean = pickle.load(pickle_in)\n",
        "  pickle_in.close()\n",
        "\n",
        "  test_X = result_test.copy()\n",
        "  for c in test_X.columns:\n",
        "    test_X[c] = (test_X[[c]].merge(data_frame[c], on = c, how = 'left'))[\"target\"]\n",
        "  test_X = test_X.fillna(global_mean)\n",
        "  test_target = test_X.copy()\n",
        "\n",
        "  print('Adding SVD transformed features to test data.....')\n",
        "  ## Declaring a dictionary for SVD features\n",
        "  cv= {} # Dictionary for CountVectorizer\n",
        "  svd = {} # Dictionary for SVD result, n_component = 1\n",
        "  result_test_svd_tfidf = pd.DataFrame()\n",
        "\n",
        "\n",
        "  list_test = []\n",
        "  for name in list_col:\n",
        "    val2 = 'X_test' + '_' + name \n",
        "    list_test.append(val2)\n",
        "\n",
        "  X_test = result_test.copy()\n",
        "\n",
        "  from sklearn.decomposition import TruncatedSVD\n",
        "  from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "  pickle_in = open(\"vectorizer.pickle\",\"rb\")\n",
        "  vectorizer = pickle.load(pickle_in)\n",
        "  pickle_in.close()\n",
        "\n",
        "  pickle_in = open(\"dim_red.pickle\",\"rb\")\n",
        "  dim_red = pickle.load(pickle_in)\n",
        "  pickle_in.close()\n",
        "\n",
        "  for index,name in enumerate(list_test):\n",
        "      name = name[7:]\n",
        "      svd_name = name + '_svd'\n",
        "      cv[list_test[index]] = vectorizer[index].transform(X_test[name].values)\n",
        "      # Using Truncated SVD\n",
        "      n_components=1\n",
        "      svd[list_test[index]] = dim_red[index].transform(cv[list_test[index]])\n",
        "      # Storing the SVD in the dataframe\n",
        "      for i in range(n_components):\n",
        "          k=str(i)\n",
        "          result_test_svd_tfidf[svd_name+'_'+ k] = svd[list_test[index]][:,i]\n",
        "\n",
        "  print('Concatinating target encoded features, frequency encoded features and SVD transformed features....')\n",
        "  ## Concatinating the dataframes\n",
        "  result_test_final = pd.concat([test_target, result_test_svd_tfidf, test_freq], axis=1)\n",
        "  pickle_out = open(\"df.pickle\",\"wb\")\n",
        "  pickle.dump(result_test_final, pickle_out)\n",
        "  pickle_out.close()\n",
        "\n",
        "  X_te = result_test_final.values\n",
        "\n",
        "  print('Prediction result using the Saved ML model')\n",
        "\n",
        "  # Load the Model back from file\n",
        "  Pkl_Filename = \"model_new_xgb.pkl\"  \n",
        "  with open(Pkl_Filename, 'rb') as file:  \n",
        "      model = pickle.load(file)\n",
        "  \n",
        "  predictions = model.predict_proba(X_te)[:,1]\n",
        "  print('The Predicted value from the ML model is : ', predictions)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHIOEDD1S9yv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91e8cf7e-4eb5-41cf-b185-3c891c605791"
      },
      "source": [
        "final_fun_1(df)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data preprocessing- Pair Wise featurization (Pairs of 2 and Pairs of 3).....\n",
            "Frequency Encoding the test data......\n",
            "Target Encoding test data....\n",
            "Adding SVD transformed features to test data.....\n",
            "Concatinating target encoded features, frequency encoded features and SVD transformed features....\n",
            "Prediction result using the Saved ML model\n",
            "The Predicted value from the ML model is :  [0.9937163  0.26806566]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phiOtwc4YRt8"
      },
      "source": [
        "# https://stackoverflow.com/questions/58020251/how-to-save-classifier-in-sklearn-with-countvectorizer-and-tfidftransformer\n",
        "def function_2(X,Y):\n",
        "\n",
        "  df = pd.DataFrame(X, columns = ['RESOURCE', 'MGR_ID', 'ROLE_ROLLUP_1', 'ROLE_ROLLUP_2', 'ROLE_DEPTNAME','ROLE_TITLE','ROLE_FAMILY_DESC', 'ROLE_FAMILY', 'ROLE_CODE'])\n",
        "  result_test = df.drop(columns=['ROLE_TITLE'])\n",
        "\n",
        "  print('Data preprocessing- Pair Wise featurization (Pairs of 2 and Pairs of 3).....')\n",
        "  # https://stackoverflow.com/questions/43347939/all-possible-combinations-of-columns-in-dataframe-pandas-python\n",
        "  # this helper function combines columns in pairs of 2. We will find out if this feature engineering technique creates a better ML model\n",
        "  # the result of pairing is columnX_columnY\n",
        "  def group_data_2(res):\n",
        "    df = res\n",
        "    cc = list(combinations(df.columns,2))\n",
        "    df = pd.concat([df[c[0]] + '_' + df[c[1]] for c in cc], axis=1, keys=cc)\n",
        "    df.columns = df.columns.map('_'.join)\n",
        "    return df\n",
        "\n",
        "  # this function combines columns in pair of 3\n",
        "  # the result of pairing is columnX_columnY_columnZ\n",
        "  def group_data_3(res):\n",
        "    df = res\n",
        "    cc = list(combinations(df.columns,3))\n",
        "    df = pd.concat([df[c[0]] + '_' + df[c[1]] + '_' + df[c[2]] for c in cc], axis=1, keys=cc)\n",
        "    df.columns = df.columns.map('_'.join)\n",
        "    return df\n",
        "\n",
        "\n",
        "  # Pair-wise featurization of Test data\n",
        "  # feature engineering: combining columns in pairs of 2\n",
        "  # the result is columnX_columnY and then concatinating all the singular columns with the resultant pairwise combination.\n",
        "  result2 = group_data_2(result_test)\n",
        "  result3 = group_data_3(result_test)\n",
        "  frames = [result_test,result2,result3]\n",
        "  result_test = pd.concat(frames,axis=1)\n",
        "\n",
        "  # The total columns should be 8C2 + 8C3 + 8 = 92\n",
        "\n",
        "  # Storing the column names in a list, we will use this later in SVD and countVectorization\n",
        "  list_col = list(result_test.columns)\n",
        "\n",
        "  print(\"Frequency Encoding the test data......\")\n",
        "  pickle_in = open(\"freq_frame.pickle\",\"rb\")\n",
        "  data_frame = pickle.load(pickle_in)\n",
        "  pickle_in.close()\n",
        "\n",
        "  test_X = result_test.copy()\n",
        "  new_dataset = pd.DataFrame()\n",
        "  for col in test_X.columns:\n",
        "    freq = (test_X[[col]].merge(data_frame[col], on = col, how = 'left'))[col+\"_freq\"]\n",
        "    new_dataset[col+\"_freq\"] = freq\n",
        "  new_dataset = new_dataset.fillna(0)\n",
        "  test_freq = new_dataset.copy()\n",
        "\n",
        "  print('Target Encoding test data....')\n",
        "  pickle_in = open(\"data_frame.pickle\",\"rb\")\n",
        "  data_frame = pickle.load(pickle_in)\n",
        "  pickle_in.close()\n",
        "\n",
        "  pickle_in = open(\"global_mean.pickle\",\"rb\")\n",
        "  global_mean = pickle.load(pickle_in)\n",
        "  pickle_in.close()\n",
        "\n",
        "  test_X = result_test.copy()\n",
        "  for c in test_X.columns:\n",
        "    test_X[c] = (test_X[[c]].merge(data_frame[c], on = c, how = 'left'))[\"target\"]\n",
        "  test_X = test_X.fillna(global_mean)\n",
        "  test_target = test_X.copy()\n",
        "\n",
        "  print('Adding SVD features to test data.....')\n",
        "  ## Declaring a dictionary for SVD features\n",
        "  cv= {} # Dictionary for CountVectorizer\n",
        "  svd = {} # Dictionary for SVD result, n_component = 1\n",
        "  result_test_svd_tfidf = pd.DataFrame()\n",
        "\n",
        "\n",
        "  list_test = []\n",
        "  for name in list_col:\n",
        "    val2 = 'X_test' + '_' + name \n",
        "    list_test.append(val2)\n",
        "\n",
        "  X_test = result_test.copy()\n",
        "\n",
        "  from sklearn.decomposition import TruncatedSVD\n",
        "  from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "  pickle_in = open(\"vectorizer.pickle\",\"rb\")\n",
        "  vectorizer = pickle.load(pickle_in)\n",
        "  pickle_in.close()\n",
        "\n",
        "  pickle_in = open(\"dim_red.pickle\",\"rb\")\n",
        "  dim_red = pickle.load(pickle_in)\n",
        "  pickle_in.close()\n",
        "\n",
        "  for index,name in enumerate(list_test):\n",
        "      name = name[7:]\n",
        "      svd_name = name + '_svd'\n",
        "      cv[list_test[index]] = vectorizer[index].transform(X_test[name].values)\n",
        "      # Using Truncated SVD\n",
        "      n_components=1\n",
        "      svd[list_test[index]] = dim_red[index].transform(cv[list_test[index]])\n",
        "      # Storing the SVD in the dataframe\n",
        "      for i in range(n_components):\n",
        "          k=str(i)\n",
        "          result_test_svd_tfidf[svd_name+'_'+ k] = svd[list_test[index]][:,i]\n",
        "\n",
        "  print('Concatinating Response coded features with SVD features....')\n",
        "  ## Concatinating the dataframes\n",
        "  result_test_final = pd.concat([test_target, result_test_svd_tfidf, test_freq], axis=1)\n",
        "  pickle_out = open(\"df.pickle\",\"wb\")\n",
        "  pickle.dump(result_test_final, pickle_out)\n",
        "  pickle_out.close()\n",
        "\n",
        "  X_te = result_test_final.values\n",
        "\n",
        "  print('Prediction result using the Saved ML model')\n",
        "  # Machine-Learning Model\n",
        "\n",
        "  # Load the Model back from file\n",
        "  Pkl_Filename = \"model_new_xgb.pkl\"  \n",
        "  with open(Pkl_Filename, 'rb') as file:  \n",
        "      model = pickle.load(file)\n",
        "  \n",
        "  predictions = model.predict_proba(X_te)[:,1]\n",
        "  print('The Predicted value from the ML model is : ', predictions)\n",
        "\n",
        "  fpr, tpr, thresholds = metrics.roc_curve(Y, predictions, pos_label=1)\n",
        "  auc_value = metrics.auc(fpr,tpr)\n",
        "\n",
        "  return auc_value\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYlwhSElCfBe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "62a973b6-1551-43e7-9f87-9fbbce12447e"
      },
      "source": [
        "auc_value = function_2(df,[1, 0])\n",
        "print('auc value is: ', auc_value )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data preprocessing- Pair Wise featurization (Pairs of 2 and Pairs of 3).....\n",
            "Frequency Encoding the test data......\n",
            "Target Encoding test data....\n",
            "Adding SVD features to test data.....\n",
            "Concatinating Response coded features with SVD features....\n",
            "Prediction result using the Saved ML model\n",
            "The Predicted value from the ML model is :  [0.9937163  0.26806566]\n",
            "auc value is:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}